{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compute_answers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn66ReqT_lu8",
        "outputId": "9186360e-4c28-43e0-eb46-1c2ab6cff879"
      },
      "source": [
        "# Library to read json\n",
        "import json\n",
        "\n",
        "# Numeric and data manipulation tools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Deep learning framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Natural language tools\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "# Other tools\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import OrderedDict, Counter\n",
        "from time import time\n",
        "from itertools import zip_longest\n",
        "\n",
        "# automatic mixed precision training:\n",
        "from torch.cuda.amp import autocast \n",
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "# Type hint\n",
        "from typing import Optional, Callable, Tuple, Dict, List, Union\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use GPU acceleration if possible\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using this device:', DEVICE)\n",
        "   \n",
        "# to avoid memory problems:\n",
        "torch.backends.cudnn.enabled = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Using this device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4AkD48GAWgV"
      },
      "source": [
        "# Define special tokens\n",
        "PAD = '<PAD>'\n",
        "UNK = '<UNK>'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB15xvrsAaQY"
      },
      "source": [
        "# Lambda for computing the mean of a list\n",
        "mean: Callable[[List[float]], float] = lambda l: sum(l) / len(l)\n",
        "\n",
        "# Lambda for transforming a list of tuples into a tuple of lists\n",
        "to_tuple_of_lists: Callable[[List[Tuple]], Tuple[List]] = lambda list_of_tuples: tuple(map(list, zip(*list_of_tuples)))\n",
        "\n",
        "# Lambda for transforming a tuple of lists into a list of tuples\n",
        "to_list_of_tuples: Callable[[Tuple[List]], List[Tuple]] = lambda tuple_of_lists: list(zip(*tuple_of_lists))\n",
        "\n",
        "# Lambda for iterating with batches (if the length of the sequences does not match with the batch size, tuples of empty lists are appended)\n",
        "batch_iteration: Callable[[List[Tuple]], zip] = lambda data, batch_size: zip_longest(*[iter(data)] * batch_size, fillvalue=([], [], []))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxaoDcwuAg21"
      },
      "source": [
        "\"\"\"\n",
        "json structure:\n",
        "\n",
        "data []\n",
        "|---title\n",
        "|---paragraphs []\n",
        "|   |---context\n",
        "|   |---qas []\n",
        "|   |   |---question\n",
        "|   |   |---id\n",
        "version\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "filename = 'training_set.json'\n",
        "\n",
        "with open(filename, 'r') as f:\n",
        "    raw_data = f.readlines()[0]\n",
        "\n",
        "parsed_data = json.loads(raw_data)['data']\n",
        "\n",
        "context_list = []\n",
        "context_index = -1\n",
        "paragraph_index = -1\n",
        "\n",
        "dataset = {'paragraph_index': [], 'context_index': [], 'question': [], 'id': []}\n",
        "\n",
        "for i in range(len(parsed_data)):\n",
        "    paragraph_index += 1\n",
        "    for j in range(len(parsed_data[i]['paragraphs'])):\n",
        "        context_list.append(parsed_data[i]['paragraphs'][j]['context'])\n",
        "        context_index += 1\n",
        "\n",
        "        for k in range(len(parsed_data[i]['paragraphs'][j]['qas'])):\n",
        "            question = parsed_data[i]['paragraphs'][j]['qas'][k]['question']\n",
        "            id = parsed_data[i]['paragraphs'][j]['qas'][k]['id']\n",
        "\n",
        "            dataset['paragraph_index'].append(paragraph_index)\n",
        "            dataset['context_index'].append(context_index)\n",
        "            dataset['question'].append(question)\n",
        "            dataset['id'].append(id)\n",
        "\n",
        "df = pd.DataFrame.from_dict(dataset)\n",
        "id_list = df['id'].tolist()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w81pKZmOBJP8",
        "outputId": "53e211cb-c600-4761-ee81-2a9ebb8ca1c5"
      },
      "source": [
        "# Embeddings\n",
        "emb_dim = 50\n",
        "glove_model = gloader.load('glove-wiki-gigaword-' + str(emb_dim))\n",
        "print('\\nDownload completed.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "\n",
            "Download completed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izsVax51mnV5"
      },
      "source": [
        "# Solo per Giacomo e Lorenzo\n",
        "# Fixing \"ValueError: substring not found\"\n",
        "# https://github.com/nltk/nltk/issues/1750\n",
        "to_replace = {'\"': ' ', '\\'': ' ', '`': ' '}\n",
        "\n",
        "def replace_all(text):\n",
        "    for i, j in to_replace.items():\n",
        "        text = text.replace(i, j)\n",
        "\n",
        "    return text\n",
        "\n",
        "context_list = [replace_all(context) for context in context_list]\n",
        "df['question'] = df['question'].apply(lambda x: replace_all(x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXDDcW3GqueS"
      },
      "source": [
        "# DEBUG ONLY\n",
        "if True:\n",
        "    df = df[:100]\n",
        "    id_list = df['id'].tolist()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhohVuzFBQl5"
      },
      "source": [
        "def tokenize_corpus(df: pd.DataFrame, context_list: List[str]):\n",
        "    twt = TreebankWordTokenizer()\n",
        "    \n",
        "    # Retrieve contexts\n",
        "    contexts = df['context_index'].apply(lambda x: context_list[x])\n",
        "    # Tokenize both contexts and queries\n",
        "    ctx = contexts.apply(lambda x: twt.tokenize(x)).tolist()\n",
        "    qry = df['question'].apply(lambda x: twt.tokenize(x)).tolist()\n",
        "    \n",
        "    # Get spans of tokens, to revert the tokenization\n",
        "    spans_list = contexts.apply(lambda x: twt.span_tokenize(x)).tolist()\n",
        "    \n",
        "    return ctx, qry, spans_list\n",
        "\n",
        "def build_vocabulary(corpus: List[List[str]],\n",
        "                     old_word_listing: Optional[List[str]] = None) -> (Dict[int, str], Dict[int, str], List[str]):\n",
        "    flat_tokens = [x for sub in corpus for x in sub]\n",
        "    \n",
        "    if old_word_listing is None:  # standard case\n",
        "        word_listing = [PAD] + list(OrderedDict.fromkeys(flat_tokens))\n",
        "    else:  # case in which we extend an already existing vocabulary\n",
        "        word_listing = list(OrderedDict.fromkeys(old_word_listing + flat_tokens))\n",
        "        \n",
        "    idx_to_word = {i: w for i, w in enumerate(word_listing)}\n",
        "    word_to_idx = {w: i for i, w in enumerate(word_listing)}\n",
        "\n",
        "    return idx_to_word, word_to_idx, word_listing\n",
        "\n",
        "# Tokenize corpus\n",
        "context_tokenized, query_tokenized, spans_list = tokenize_corpus(df, context_list)\n",
        "corpus = context_tokenized + query_tokenized\n",
        "\n",
        "# Get word and char mappings\n",
        "i2w, w2i, wl = build_vocabulary(corpus)\n",
        "\n",
        "# OOV words\n",
        "oov_words = [word for word in wl if word not in glove_model.vocab and word != PAD]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP6_RiEZCbUT"
      },
      "source": [
        "def build_char_embedding_matrix(corpus: List[str],\n",
        "                                enc_dim: Optional[int] = 100):\n",
        "    # Flatten to obtain single characters\n",
        "    flat_chars = [c for sent in corpus for word in sent for c in word]\n",
        "    \n",
        "    # Sort characters by occurrences\n",
        "    unique_chars = Counter(flat_chars)\n",
        "    char_listing = sorted(unique_chars, key=unique_chars.get, reverse=True)\n",
        "    # Select only the enc_dim most frequent ones\n",
        "    if len(char_listing) > enc_dim - 1:\n",
        "        char_listing = char_listing[:enc_dim - 1]\n",
        "    char_listing = [PAD] + char_listing + [UNK]  # add PAD and UNK tokens\n",
        "    \n",
        "    idx_to_char = {i: c for i, c in enumerate(char_listing)}\n",
        "    char_to_idx = {c: i for i, c in enumerate(char_listing)}\n",
        "    \n",
        "    # Create one-hot vectors, reserving the last one for UNK (0...0, 1)\n",
        "    one_hot_chars = np.zeros((len(char_listing) - 1, enc_dim))\n",
        "    np.fill_diagonal(one_hot_chars, 1)\n",
        "    one_hot_chars = np.vstack([np.zeros((1, enc_dim)), one_hot_chars])  # stack zero vector on top for PAD\n",
        "    \n",
        "    return idx_to_char, char_to_idx, char_listing, one_hot_chars\n",
        "\n",
        "# Build char embedding matrix based only on the training set, and use it for validation and test too:\n",
        "# in fact, we can assume that characters appear uniformly in the three splits;\n",
        "# for those rare case in which this does not happen, we assign the UNK vector\n",
        "i2c, c2i, cl, char_emb_mtx = build_char_embedding_matrix(corpus)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SZ98e3LDDA8"
      },
      "source": [
        "# Load the model\n",
        "model = torch.load(\"bidaf_test.pt\")\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0tynVfHjeDN"
      },
      "source": [
        "from model.tensor_maker import TensorMaker\n",
        "\n",
        "tensor_maker = TensorMaker(w2i, c2i, DEVICE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTmXNvgvsU5F"
      },
      "source": [
        "# Retrieve original contexts\n",
        "contexts = df['context_index'].apply(lambda x: context_list[x])\n",
        "\n",
        "evaluation_data = []\n",
        "for i in range(len(context_tokenized)):\n",
        "    evaluation_data.append((contexts[i], context_tokenized[i], query_tokenized[i]))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6gGQMM8EBfN"
      },
      "source": [
        "def generate_evaluation_json(model,\n",
        "                             evaluation_data: List[Tuple[str, List[str], List[str]]],\n",
        "                             spans_list: List[List[Tuple[int, int]]],\n",
        "                             id_list: List[str],\n",
        "                             filename: str):\n",
        "    predictions = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_size = 4\n",
        "\n",
        "        # Create batch iterator\n",
        "        batch_iter = batch_iteration(evaluation_data, batch_size)\n",
        "\n",
        "        for i, batch in enumerate(batch_iter):\n",
        "            # Extract samples\n",
        "            batch_context, batch_context_tokenized, batch_query_tokenized = to_tuple_of_lists(batch)\n",
        "\n",
        "            # Filter valid samples in batches (in case of incomplete ones)\n",
        "            batch_context_tokenized: Tuple[List[str]] = tuple([c for c in batch_context_tokenized if len(c) > 0])\n",
        "            batch_query_tokenized: Tuple[List[str]] = tuple([q for q in batch_query_tokenized if len(q) > 0])\n",
        "\n",
        "            context_word_tensor, context_char_tensor, context_lengths = tensor_maker.get_tensor(batch_context_tokenized)\n",
        "            query_word_tensor, query_char_tensor, query_lengths = tensor_maker.get_tensor(batch_query_tokenized)\n",
        "\n",
        "            # Make prediction\n",
        "            p_soft_start, p_soft_end = model(context_word_tensor, context_char_tensor,\n",
        "                                             query_word_tensor, query_char_tensor)\n",
        "\n",
        "            # Argmax\n",
        "            p_start = torch.argmax(p_soft_start, dim=1)\n",
        "            p_end = torch.argmax(p_soft_end, dim=1)\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                start_word_idx = p_start[j].item()\n",
        "                end_word_idx = p_end[j].item()\n",
        "\n",
        "                span = spans_list[i * batch_size + j]\n",
        "                start_char_idx = span[start_word_idx][0]\n",
        "                end_char_idx = span[end_word_idx][1]\n",
        "\n",
        "                answer = batch_context[j][start_char_idx:end_char_idx+1]\n",
        "\n",
        "                # DEBUG: original context answer vs tokenized context answer\n",
        "                # They should match (in terms of meaning/words)!\n",
        "                print(answer)\n",
        "                print(batch_context_tokenized[j][start_word_idx:end_word_idx])\n",
        "                print()\n",
        "\n",
        "                id = id_list[i * batch_size + j]\n",
        "                predictions[id] = answer\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(json.dumps(predictions))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiOL8k7npOBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496cecaa-e6f6-4793-e1fb-026d3fb2db56"
      },
      "source": [
        "generate_evaluation_json(model, evaluation_data, spans_list, id_list, \"predictions.txt\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Main Building s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend  Venite Ad Me Omnes \n",
            "['Main', 'Building', 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', 'Venite', 'Ad', 'Me']\n",
            "\n",
            "Main Building s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend  Venite Ad Me Omnes \n",
            "['Main', 'Building', 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', 'Venite', 'Ad', 'Me']\n",
            "\n",
            "Main Building s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend  Venite Ad Me Omnes \n",
            "['Main', 'Building', 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', 'Venite', 'Ad', 'Me']\n",
            "\n",
            "Main Building s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend  Venite Ad Me Omnes \n",
            "['Main', 'Building', 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', 'Venite', 'Ad', 'Me']\n",
            "\n",
            "Omnes \n",
            "[]\n",
            "\n",
            "Observer \n",
            "[]\n",
            "\n",
            "Observer \n",
            "[]\n",
            "\n",
            "Observer \n",
            "[]\n",
            "\n",
            "Observer \n",
            "[]\n",
            "\n",
            "Observer \n",
            "[]\n",
            "\n",
            "Holy \n",
            "[]\n",
            "\n",
            "Holy \n",
            "[]\n",
            "\n",
            "Holy \n",
            "[]\n",
            "\n",
            "Holy \n",
            "[]\n",
            "\n",
            "Holy \n",
            "[]\n",
            "\n",
            "degrees offered. \n",
            "['degrees']\n",
            "\n",
            "degrees offered. \n",
            "['degrees']\n",
            "\n",
            "degrees offered. \n",
            "['degrees']\n",
            "\n",
            "degrees offered. \n",
            "['degrees']\n",
            "\n",
            "degrees offered. \n",
            "['degrees']\n",
            "\n",
            "program. \n",
            "[]\n",
            "\n",
            "program. \n",
            "[]\n",
            "\n",
            "program. \n",
            "[]\n",
            "\n",
            "program. \n",
            "[]\n",
            "\n",
            "degrees. \n",
            "[]\n",
            "\n",
            "degrees. \n",
            "[]\n",
            "\n",
            "degrees. \n",
            "[]\n",
            "\n",
            "degrees. \n",
            "[]\n",
            "\n",
            "degrees. \n",
            "[]\n",
            "\n",
            "PhD,\n",
            "[]\n",
            "\n",
            "PhD,\n",
            "[]\n",
            "\n",
            "PhD,\n",
            "[]\n",
            "\n",
            "PhD,\n",
            "[]\n",
            "\n",
            "PhD,\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "19.7%\n",
            "[]\n",
            "\n",
            "19.7%\n",
            "[]\n",
            "\n",
            "19.7%\n",
            "[]\n",
            "\n",
            "19.7%\n",
            "[]\n",
            "\n",
            "19.7%\n",
            "[]\n",
            "\n",
            "Colleges 2016. \n",
            "['Colleges']\n",
            "\n",
            "Colleges 2016. \n",
            "['Colleges']\n",
            "\n",
            "Colleges 2016. \n",
            "['Colleges']\n",
            "\n",
            "Colleges 2016. \n",
            "['Colleges']\n",
            "\n",
            "Colleges 2016. \n",
            "['Colleges']\n",
            "\n",
            "Chemistry \n",
            "[]\n",
            "\n",
            "Chemistry \n",
            "[]\n",
            "\n",
            "Chemistry \n",
            "[]\n",
            "\n",
            "Chemistry \n",
            "[]\n",
            "\n",
            "Chemistry \n",
            "[]\n",
            "\n",
            "Professor Jerome Green \n",
            "['Professor', 'Jerome']\n",
            "\n",
            "Professor Jerome Green \n",
            "['Professor', 'Jerome']\n",
            "\n",
            "Professor Jerome Green \n",
            "['Professor', 'Jerome']\n",
            "\n",
            "Professor Jerome Green \n",
            "['Professor', 'Jerome']\n",
            "\n",
            "Professor Jerome Green \n",
            "['Professor', 'Jerome']\n",
            "\n",
            "This \n",
            "[]\n",
            "\n",
            "This \n",
            "[]\n",
            "\n",
            "This \n",
            "[]\n",
            "\n",
            "This \n",
            "[]\n",
            "\n",
            "This \n",
            "[]\n",
            "\n",
            "For 44 years, the Review \n",
            "['For', '44', 'years', ',', 'the']\n",
            "\n",
            "For 44 years, the Review \n",
            "['For', '44', 'years', ',', 'the']\n",
            "\n",
            "For 44 years, the Review \n",
            "['For', '44', 'years', ',', 'the']\n",
            "\n",
            "For 44 years, the Review \n",
            "['For', '44', 'years', ',', 'the']\n",
            "\n",
            "address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational \n",
            "['address.', 'The', 'university', 'has', 'many', 'multi-disciplinary', 'institutes', 'devoted', 'to', 'research', 'in', 'varying', 'fields', ',', 'including', 'the', 'Medieval', 'Institute', ',', 'the', 'Kellogg', 'Institute', 'for', 'International', 'Studies', ',', 'the', 'Kroc', 'Institute', 'for', 'International', 'Peace', 'studies', ',', 'and', 'the', 'Center', 'for', 'Social', 'Concerns.', 'Recent', 'research', 'includes', 'work', 'on', 'family', 'conflict', 'and', 'child', 'development', ',', 'genome', 'mapping', ',', 'the', 'increasing', 'trade', 'deficit', 'of', 'the', 'United', 'States', 'with', 'China', ',', 'studies', 'in', 'fluid', 'mechanics', ',']\n",
            "\n",
            "address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational \n",
            "['address.', 'The', 'university', 'has', 'many', 'multi-disciplinary', 'institutes', 'devoted', 'to', 'research', 'in', 'varying', 'fields', ',', 'including', 'the', 'Medieval', 'Institute', ',', 'the', 'Kellogg', 'Institute', 'for', 'International', 'Studies', ',', 'the', 'Kroc', 'Institute', 'for', 'International', 'Peace', 'studies', ',', 'and', 'the', 'Center', 'for', 'Social', 'Concerns.', 'Recent', 'research', 'includes', 'work', 'on', 'family', 'conflict', 'and', 'child', 'development', ',', 'genome', 'mapping', ',', 'the', 'increasing', 'trade', 'deficit', 'of', 'the', 'United', 'States', 'with', 'China', ',', 'studies', 'in', 'fluid', 'mechanics', ',']\n",
            "\n",
            "address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational \n",
            "['address.', 'The', 'university', 'has', 'many', 'multi-disciplinary', 'institutes', 'devoted', 'to', 'research', 'in', 'varying', 'fields', ',', 'including', 'the', 'Medieval', 'Institute', ',', 'the', 'Kellogg', 'Institute', 'for', 'International', 'Studies', ',', 'the', 'Kroc', 'Institute', 'for', 'International', 'Peace', 'studies', ',', 'and', 'the', 'Center', 'for', 'Social', 'Concerns.', 'Recent', 'research', 'includes', 'work', 'on', 'family', 'conflict', 'and', 'child', 'development', ',', 'genome', 'mapping', ',', 'the', 'increasing', 'trade', 'deficit', 'of', 'the', 'United', 'States', 'with', 'China', ',', 'studies', 'in', 'fluid', 'mechanics', ',']\n",
            "\n",
            "address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational \n",
            "['address.', 'The', 'university', 'has', 'many', 'multi-disciplinary', 'institutes', 'devoted', 'to', 'research', 'in', 'varying', 'fields', ',', 'including', 'the', 'Medieval', 'Institute', ',', 'the', 'Kellogg', 'Institute', 'for', 'International', 'Studies', ',', 'the', 'Kroc', 'Institute', 'for', 'International', 'Peace', 'studies', ',', 'and', 'the', 'Center', 'for', 'Social', 'Concerns.', 'Recent', 'research', 'includes', 'work', 'on', 'family', 'conflict', 'and', 'child', 'development', ',', 'genome', 'mapping', ',', 'the', 'increasing', 'trade', 'deficit', 'of', 'the', 'United', 'States', 'with', 'China', ',', 'studies', 'in', 'fluid', 'mechanics', ',']\n",
            "\n",
            "computational \n",
            "[]\n",
            "\n",
            "March 2015[update] The Princeton Review \n",
            "['March', '2015', '[', 'update', ']', 'The', 'Princeton']\n",
            "\n",
            "March 2015[update] The Princeton Review \n",
            "['March', '2015', '[', 'update', ']', 'The', 'Princeton']\n",
            "\n",
            "March 2015[update] The Princeton Review \n",
            "['March', '2015', '[', 'update', ']', 'The', 'Princeton']\n",
            "\n",
            "March 2015[update] The Princeton Review \n",
            "['March', '2015', '[', 'update', ']', 'The', 'Princeton']\n",
            "\n",
            "March 2015[update] The Princeton Review \n",
            "['March', '2015', '[', 'update', ']', 'The', 'Princeton']\n",
            "\n",
            "dorms. \n",
            "[]\n",
            "\n",
            "dorms. \n",
            "[]\n",
            "\n",
            "dorms. \n",
            "[]\n",
            "\n",
            "dorms. \n",
            "[]\n",
            "\n",
            "dorms. \n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "\n",
            "[]\n",
            "\n",
            "Student \n",
            "[]\n",
            "\n",
            "Student \n",
            "[]\n",
            "\n",
            "Student \n",
            "[]\n",
            "\n",
            "Student \n",
            "[]\n",
            "\n",
            "Student \n",
            "[]\n",
            "\n",
            "reason. \n",
            "[]\n",
            "\n",
            "reason. \n",
            "[]\n",
            "\n",
            "reason. Notre Dame \n",
            "['reason.', 'Notre']\n",
            "\n",
            "reason. Notre Dame \n",
            "['reason.', 'Notre']\n",
            "\n",
            "reason. Notre Dame \n",
            "['reason.', 'Notre']\n",
            "\n",
            "Irish. Knute \n",
            "['Irish.']\n",
            "\n",
            "Irish. Knute \n",
            "['Irish.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}