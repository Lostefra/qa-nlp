{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kn66ReqT_lu8",
    "outputId": "9186360e-4c28-43e0-eb46-1c2ab6cff879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nihil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nihil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library to read json\n",
    "import json\n",
    "\n",
    "# Numeric and data manipulation tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Deep learning framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Natural language tools\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "# Other tools\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict, Counter\n",
    "from time import time\n",
    "from itertools import zip_longest\n",
    "\n",
    "# automatic mixed precision training:\n",
    "from torch.cuda.amp import autocast \n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# Type hint\n",
    "from typing import Optional, Callable, Tuple, Dict, List, Union\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Use GPU acceleration if possible\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using this device:', DEVICE)\n",
    "   \n",
    "# to avoid memory problems:\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G4AkD48GAWgV"
   },
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "PAD = '<PAD>'\n",
    "UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yB15xvrsAaQY"
   },
   "outputs": [],
   "source": [
    "# Lambda for computing the mean of a list\n",
    "mean: Callable[[List[float]], float] = lambda l: sum(l) / len(l)\n",
    "\n",
    "# Lambda for transforming a list of tuples into a tuple of lists\n",
    "to_tuple_of_lists: Callable[[List[Tuple]], Tuple[List]] = lambda list_of_tuples: tuple(map(list, zip(*list_of_tuples)))\n",
    "\n",
    "# Lambda for transforming a tuple of lists into a list of tuples\n",
    "to_list_of_tuples: Callable[[Tuple[List]], List[Tuple]] = lambda tuple_of_lists: list(zip(*tuple_of_lists))\n",
    "\n",
    "# Lambda for iterating with batches (if the length of the sequences does not match with the batch size, tuples of empty lists are appended)\n",
    "batch_iteration: Callable[[List[Tuple]], zip] = lambda data, batch_size: zip_longest(*[iter(data)] * batch_size, fillvalue=([], [], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UxaoDcwuAg21"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "json structure:\n",
    "\n",
    "data []\n",
    "|---title\n",
    "|---paragraphs []\n",
    "|   |---context\n",
    "|   |---qas []\n",
    "|   |   |---question\n",
    "|   |   |---id\n",
    "version\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "filename = 'training_set.json'\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    raw_data = f.readlines()[0]\n",
    "\n",
    "parsed_data = json.loads(raw_data)['data']\n",
    "\n",
    "context_list = []\n",
    "context_index = -1\n",
    "paragraph_index = -1\n",
    "\n",
    "dataset = {'paragraph_index': [], 'context_index': [], 'question': [], 'id': []}\n",
    "\n",
    "for i in range(len(parsed_data)):\n",
    "    paragraph_index += 1\n",
    "    for j in range(len(parsed_data[i]['paragraphs'])):\n",
    "        context_list.append(parsed_data[i]['paragraphs'][j]['context'])\n",
    "        context_index += 1\n",
    "\n",
    "        for k in range(len(parsed_data[i]['paragraphs'][j]['qas'])):\n",
    "            question = parsed_data[i]['paragraphs'][j]['qas'][k]['question']\n",
    "            id = parsed_data[i]['paragraphs'][j]['qas'][k]['id']\n",
    "\n",
    "            dataset['paragraph_index'].append(paragraph_index)\n",
    "            dataset['context_index'].append(context_index)\n",
    "            dataset['question'].append(question)\n",
    "            dataset['id'].append(id)\n",
    "\n",
    "df = pd.DataFrame.from_dict(dataset)\n",
    "id_list = df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w81pKZmOBJP8",
    "outputId": "53e211cb-c600-4761-ee81-2a9ebb8ca1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download completed.\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "emb_dim = 50\n",
    "glove_model = gloader.load('glove-wiki-gigaword-' + str(emb_dim))\n",
    "print('\\nDownload completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XhohVuzFBQl5"
   },
   "outputs": [],
   "source": [
    "def tokenize_corpus(df: pd.DataFrame, context_list: List[str]):\n",
    "    twt = TreebankWordTokenizer()\n",
    "    \n",
    "    # Retrieve contexts\n",
    "    contexts = df['context_index'].apply(lambda x: context_list[x])\n",
    "    # Tokenize both contexts and queries\n",
    "    ctx = contexts.apply(lambda x: twt.tokenize(x)).tolist()\n",
    "    qry = df['question'].apply(lambda x: twt.tokenize(x)).tolist()\n",
    "    \n",
    "    # Get spans of tokens, to revert the tokenization\n",
    "    spans_list = contexts.apply(lambda x: list(twt.span_tokenize(x))).tolist()\n",
    "    \n",
    "    return ctx, qry, spans_list\n",
    "\n",
    "# Tokenize corpus\n",
    "context_tokenized, query_tokenized, spans_list = tokenize_corpus(df, context_list)\n",
    "corpus = context_tokenized + query_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiDAF(\n",
       "  (char_embedder): CharEmbedder(\n",
       "    (embedding): Embedding(101, 100)\n",
       "    (conv_layer): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "    (fc1): Linear(in_features=100, out_features=64, bias=False)\n",
       "    (fc2): Linear(in_features=64, out_features=50, bias=False)\n",
       "  )\n",
       "  (train_word_embedder): WordEmbedder(\n",
       "    (embedding): Embedding(114839, 50)\n",
       "  )\n",
       "  (eval_word_embedder): WordEmbedder(\n",
       "    (embedding): Embedding(131821, 50)\n",
       "  )\n",
       "  (highway_net): ConvolutionalHighwayNetwork(\n",
       "    (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (gate1): Conv2d(1, 1, kernel_size=(5, 100), stride=(1, 1), padding=(2, 0))\n",
       "    (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (gate2): Conv2d(1, 1, kernel_size=(5, 100), stride=(1, 1), padding=(2, 0))\n",
       "  )\n",
       "  (ctx_rnn): GRU(100, 100, batch_first=True, bidirectional=True)\n",
       "  (w_s): Linear(in_features=600, out_features=1, bias=False)\n",
       "  (mod_rnn): GRU(800, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (w_p_start): Linear(in_features=1000, out_features=1, bias=False)\n",
       "  (p_end_rnn): GRU(201, 100, batch_first=True, bidirectional=True)\n",
       "  (w_p_end): Linear(in_features=1000, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.bidaf import BiDAF\n",
    "from model.char_embedder import CharEmbedder\n",
    "from model.word_embedder import WordEmbedder\n",
    "from model.tensor_maker import TensorMaker\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Import from pickle files\n",
    "char_embedder = None\n",
    "with open(os.path.join('best_model', 'char_emb.pickle'), 'rb') as f:\n",
    "    char_embedder = pickle.load(f)\n",
    "train_word_embedder = None\n",
    "with open(os.path.join('best_model', 'train_word_embedder.pickle'), 'rb') as f:\n",
    "    train_word_embedder = pickle.load(f)\n",
    "val_word_embedder = None\n",
    "with open(os.path.join('best_model', 'val_word_embedder.pickle'), 'rb') as f:\n",
    "    val_word_embedder = pickle.load(f)\n",
    "\n",
    "# Create model\n",
    "model_bidaf = BiDAF(char_embedder, train_word_embedder, val_word_embedder, use_constraint=True, use_dropout=False).to(DEVICE)\n",
    "# Load the model state\n",
    "model_bidaf.load_state_dict(torch.load(os.path.join('best_model', 'bidaf_test.pt')))\n",
    "model_bidaf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g0tynVfHjeDN"
   },
   "outputs": [],
   "source": [
    "from model.tensor_maker import TensorMaker\n",
    "\n",
    "tensor_maker = None\n",
    "with open(os.path.join('best_model', 'tensor_maker.pickle'), 'rb') as f:\n",
    "    tensor_maker = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aTmXNvgvsU5F"
   },
   "outputs": [],
   "source": [
    "# Retrieve original contexts\n",
    "contexts = df['context_index'].apply(lambda x: context_list[x])\n",
    "\n",
    "evaluation_data = []\n",
    "for i in range(len(context_tokenized)):\n",
    "    evaluation_data.append((contexts[i], context_tokenized[i], query_tokenized[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_json(model,\n",
    "                             evaluation_data: List[Tuple[str, List[str], List[str]]],\n",
    "                             spans_list: List[List[Tuple[int, int]]],\n",
    "                             id_list: List[str],\n",
    "                             filename: str):\n",
    "    predictions = {}\n",
    " \n",
    "    with torch.no_grad():\n",
    "        batch_size = 4\n",
    " \n",
    "        # Create batch iterator\n",
    "        batch_iter = batch_iteration(evaluation_data, batch_size)\n",
    " \n",
    "        for i, batch in enumerate(batch_iter):\n",
    "            # Extract samples\n",
    "            batch_context, batch_context_tokenized, batch_query_tokenized = to_tuple_of_lists(batch)\n",
    " \n",
    "            # Filter valid samples in batches (in case of incomplete ones)\n",
    "            batch_context_tokenized: Tuple[List[str]] = tuple([c for c in batch_context_tokenized if len(c) > 0])\n",
    "            batch_query_tokenized: Tuple[List[str]] = tuple([q for q in batch_query_tokenized if len(q) > 0])\n",
    " \n",
    "            context_word_tensor, context_char_tensor, context_lengths = tensor_maker.get_tensor(batch_context_tokenized)\n",
    "            query_word_tensor, query_char_tensor, query_lengths = tensor_maker.get_tensor(batch_query_tokenized)\n",
    " \n",
    "            # Make prediction\n",
    "            p_soft_start, p_soft_end = model(context_word_tensor, context_char_tensor,\n",
    "                                             query_word_tensor, query_char_tensor)\n",
    " \n",
    "            # Argmax\n",
    "            p_start = torch.argmax(p_soft_start, dim=1)\n",
    "            p_end = torch.argmax(p_soft_end, dim=1)\n",
    " \n",
    "            for j in range(batch_size):\n",
    "                start_word_idx = p_start[j].item()\n",
    "                end_word_idx = p_end[j].item()\n",
    " \n",
    "                span = spans_list[i * batch_size + j]\n",
    "                start_char_idx = span[start_word_idx][0]\n",
    "                end_char_idx = span[end_word_idx][1]\n",
    " \n",
    "                answer = batch_context[j][start_char_idx:end_char_idx+1]\n",
    " \n",
    "                id = id_list[i * batch_size + j]\n",
    "                predictions[id] = answer\n",
    " \n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(json.dumps(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GiOL8k7npOBz",
    "outputId": "496cecaa-e6f6-4793-e1fb-026d3fb2db56"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-24-d6bdf67d82ce>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mgenerate_evaluation_json\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_bidaf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevaluation_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mspans_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mid_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"predictions.txt\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-23-df5df77e343e>\u001B[0m in \u001B[0;36mgenerate_evaluation_json\u001B[0;34m(model, evaluation_data, spans_list, id_list, filename)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0;31m# Make prediction\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m             p_soft_start, p_soft_end = model(context_word_tensor, context_char_tensor,\n\u001B[0m\u001B[1;32m     27\u001B[0m                                              query_word_tensor, query_char_tensor)\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/Storage/miniconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documenti/Università/Artificial Intelligence/Year 2/Natural Language Processing/qa-nlp/qa-nlp/model/bidaf.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, batch_context_word, batch_context_char, batch_query_word, batch_query_char)\u001B[0m\n\u001B[1;32m    103\u001B[0m         \u001B[0mp_start\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mw_p_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mm\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (bs, t)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m         \u001B[0;31m# If specified, concatenate p_start with RNN input to impose constraint\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 105\u001B[0;31m         \u001B[0mm_2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp_end_rnn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp_start\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    106\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_constraint\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp_end_rnn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (bs, t, 2d)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m         \u001B[0mp_end\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mw_p_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mm_2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (bs, t)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/Storage/miniconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/Storage/miniconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    737\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_forward_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_sizes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    738\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mbatch_sizes\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 739\u001B[0;31m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001B[0m\u001B[1;32m    740\u001B[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001B[1;32m    741\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "generate_evaluation_json(model_bidaf, evaluation_data, spans_list, id_list, \"predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "compute_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Natural Language Processing",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}