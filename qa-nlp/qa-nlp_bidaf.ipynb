{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# qa-nlp\n",
    "Question answering neural model based on the SQuAD dataset.\n",
    "\n",
    "Authors:\n",
    "- Lorenzo Mario Amorosa\n",
    "- Andrea Espis\n",
    "- Mattia Orlandi\n",
    "- Giacomo Pinardi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97and\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\97and\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library to read json\n",
    "import json\n",
    "\n",
    "# Numeric and data manipulation tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Deep learning framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Natural language tools\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "# Other tools\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict, Counter\n",
    "from time import time\n",
    "from itertools import zip_longest\n",
    "\n",
    "# automatic mixed precision training:\n",
    "from torch.cuda.amp import autocast \n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# Type hint\n",
    "from typing import Optional, Callable, Tuple, Dict, List, Union\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use GPU acceleration if possible\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using this device:', DEVICE)\n",
    "\n",
    "if not(torch.cuda.is_available()):\n",
    "    raise Exception('Switch to runtime GPU, otherwise the code won\\'t work properly')\n",
    "   \n",
    "# to avoid memory problems:\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def fix_random(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using this device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use GPU acceleration if possible\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using this device:\", DEVICE)\n",
    "\n",
    "# Define special tokens\n",
    "PAD = '<PAD>'\n",
    "UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda for computing the mean of a list\n",
    "mean: Callable[[List[float]], float] = lambda l: sum(l) / len(l)\n",
    "\n",
    "# Lambda for transforming a list of tuples into a tuple of lists\n",
    "to_tuple_of_lists: Callable[[List[Tuple]], Tuple[List]] = lambda list_of_tuples: tuple(map(list, zip(*list_of_tuples)))\n",
    "\n",
    "# Lambda for transforming a tuple of lists into a list of tuples\n",
    "to_list_of_tuples: Callable[[Tuple[List]], List[Tuple]] = lambda tuple_of_lists: list(zip(*tuple_of_lists))\n",
    "\n",
    "# Lambda for iterating with batches (if the length of the sequences does not match with the batch size, tuples of empty lists are appended)\n",
    "batch_iteration: Callable[[List[Tuple]], zip] = lambda data, batch_size: zip_longest(*[iter(data)] * batch_size, fillvalue=([], [], []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_index</th>\n",
       "      <th>context_index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>515</td>\n",
       "      <td>541</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>188</td>\n",
       "      <td>213</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>279</td>\n",
       "      <td>296</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>381</td>\n",
       "      <td>420</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>92</td>\n",
       "      <td>126</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paragraph_index  context_index  \\\n",
       "0                0              0   \n",
       "1                0              0   \n",
       "2                0              0   \n",
       "3                0              0   \n",
       "4                0              0   \n",
       "\n",
       "                                            question  answer_start  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...           515   \n",
       "1  What is in front of the Notre Dame Main Building?           188   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...           279   \n",
       "3                  What is the Grotto at Notre Dame?           381   \n",
       "4  What sits on top of the Main Building at Notre...            92   \n",
       "\n",
       "   answer_end                              answer_text  \n",
       "0         541               Saint Bernadette Soubirous  \n",
       "1         213                a copper statue of Christ  \n",
       "2         296                        the Main Building  \n",
       "3         420  a Marian place of prayer and reflection  \n",
       "4         126       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "json structure:\n",
    "\n",
    "data []\n",
    "|---title\n",
    "|---paragraphs []\n",
    "|   |---context\n",
    "|   |---qas []\n",
    "|   |   |---answers []\n",
    "|   |   |   |---answer_start\n",
    "|   |   |   |---text\n",
    "|   |   |---question\n",
    "|   |   |---id\n",
    "version\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "filename = 'training_set.json'\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    raw_data = f.readlines()[0]\n",
    "\n",
    "parsed_data = json.loads(raw_data)['data']\n",
    "\n",
    "context_list = []\n",
    "context_index = -1\n",
    "paragraph_index = -1\n",
    "\n",
    "dataset = {'paragraph_index': [], 'context_index': [], 'question': [], 'answer_start': [], 'answer_end': [], 'answer_text': []}\n",
    "\n",
    "for i in range(len(parsed_data)):\n",
    "    paragraph_index += 1\n",
    "    for j in range(len(parsed_data[i]['paragraphs'])):\n",
    "        context_list.append(parsed_data[i]['paragraphs'][j]['context'])\n",
    "        context_index += 1\n",
    "\n",
    "        for k in range(len(parsed_data[i]['paragraphs'][j]['qas'])):\n",
    "            question = parsed_data[i]['paragraphs'][j]['qas'][k]['question']\n",
    "\n",
    "            for l in range(len(parsed_data[i]['paragraphs'][j]['qas'][k]['answers'])): \n",
    "                answer_start = parsed_data[i]['paragraphs'][j]['qas'][k]['answers'][l]['answer_start']\n",
    "                answer_text = parsed_data[i]['paragraphs'][j]['qas'][k]['answers'][l]['text']\n",
    "\n",
    "                answer_end = answer_start + len(answer_text)\n",
    "\n",
    "                dataset['paragraph_index'].append(paragraph_index)\n",
    "                dataset['context_index'].append(context_index)\n",
    "                dataset['question'].append(question)\n",
    "                dataset['answer_start'].append(answer_start)\n",
    "                dataset['answer_end'].append(answer_end)\n",
    "                dataset['answer_text'].append(answer_text)\n",
    "\n",
    "df = pd.DataFrame.from_dict(dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? \n",
      "\n",
      "Context:  One of the main driving forces in the growth of the University was its football team, the Notre Dame Fighting Irish. Knute Rockne became head coach in 1918. Under Rockne, the Irish would post a record of 105 wins, 12 losses, and five ties. During his 13 years the Irish won three national championships, had five undefeated seasons, won the Rose Bowl in 1925, and produced players such as George Gipp and the \"Four Horsemen\". Knute Rockne has the highest winning percentage (.881) in NCAA Division I/FBS football history. Rockne's offenses employed the Notre Dame Box and his defenses ran a 7–2–2 scheme. The last game Rockne coached was on December 14, 1930 when he led a group of Notre Dame all-stars against the New York Giants in New York City.\n",
      "Question: In what year did the team lead by Knute Rockne win the Rose Bowl? \n",
      "\n",
      "Context:  In 1842, the Bishop of Vincennes, Célestine Guynemer de la Hailandière, offered land to Father Edward Sorin of the Congregation of the Holy Cross, on the condition that he build a college in two years. Fr. Sorin arrived on the site with eight Holy Cross brothers from France and Ireland on November 26, 1842, and began the school using Father Stephen Badin's old log chapel. He soon erected additional buildings, including Old College, the first church, and the first main building. They immediately acquired two students and set about building additions to the campus.\n",
      "Question: In what year was Father Edward Sorin given two years to create a college? \n",
      "\n",
      "Context:  Following the disbandment of Destiny's Child in June 2005, she released her second solo album, B'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.\n",
      "Question: What is the name of Beyoncé's alter-ego? \n",
      "\n",
      "Context:  Beyoncé's first solo recording was a feature on Jay Z's \"'03 Bonnie & Clyde\" that was released in October 2002, peaking at number four on the U.S. Billboard Hot 100 chart. Her first solo album Dangerously in Love was released on June 24, 2003, after Michelle Williams and Kelly Rowland had released their solo efforts. The album sold 317,000 copies in its first week, debuted atop the Billboard 200, and has since sold 11 million copies worldwide. The album's lead single, \"Crazy in Love\", featuring Jay Z, became Beyoncé's first number-one single as a solo artist in the US. The single \"Baby Boy\" also reached number one, and singles, \"Me, Myself and I\" and \"Naughty Girl\", both reached the top-five. The album earned Beyoncé a then record-tying five awards at the 46th Annual Grammy Awards; Best Contemporary R&B Album, Best Female R&B Vocal Performance for \"Dangerously in Love 2\", Best R&B Song and Best Rap/Sung Collaboration for \"Crazy in Love\", and Best R&B Performance by a Duo or Group with Vocals for \"The Closer I Get to You\" with Luther Vandross.\n",
      "Question: Beyonce's first album by herself was called what? \n",
      "\n",
      "Context:  In 2011, documents obtained by WikiLeaks revealed that Beyoncé was one of many entertainers who performed for the family of Libyan ruler Muammar Gaddafi. Rolling Stone reported that the music industry was urging them to return the money they earned for the concerts; a spokesperson for Beyoncé later confirmed to The Huffington Post that she donated the money to the Clinton Bush Haiti Fund. Later that year she became the first solo female artist to headline the main Pyramid stage at the 2011 Glastonbury Festival in over twenty years, and was named the highest-paid performer in the world per minute.\n",
      "Question: Who did Beyonce donate the money to earned from her shows? \n",
      "\n",
      "Context:  Beyoncé is believed to have first started a relationship with Jay Z after a collaboration on \"'03 Bonnie & Clyde\", which appeared on his seventh album The Blueprint 2: The Gift & The Curse (2002). Beyoncé appeared as Jay Z's girlfriend in the music video for the song, which would further fuel speculation of their relationship. On April 4, 2008, Beyoncé and Jay Z were married without publicity. As of April 2014, the couple have sold a combined 300 million records together. The couple are known for their private relationship, although they have appeared to become more relaxed in recent years. Beyoncé suffered a miscarriage in 2010 or 2011, describing it as \"the saddest thing\" she had ever endured. She returned to the studio and wrote music in order to cope with the loss. In April 2011, Beyoncé and Jay Z traveled to Paris in order to shoot the album cover for her 4, and unexpectedly became pregnant in Paris.\n",
      "Question: Who did Beyonce have a relationship with? \n",
      "\n",
      "Context:  Beyoncé's music is generally R&B, but she also incorporates pop, soul and funk into her songs. 4 demonstrated Beyoncé's exploration of 90s-style R&B, as well as further use of soul and hip hop than compared to previous releases. While she almost exclusively releases English songs, Beyoncé recorded several Spanish songs for Irreemplazable (re-recordings of songs from B'Day for a Spanish-language audience), and the re-release of B'Day. To record these, Beyoncé was coached phonetically by American record producer Rudy Perez.\n",
      "Question: What language does she mainly sing? \n",
      "\n",
      "Context:  In September 2010, Beyoncé made her runway modelling debut at Tom Ford's Spring/Summer 2011 fashion show. She was named \"World's Most Beautiful Woman\" by People and the \"Hottest Female Singer of All Time\" by Complex in 2012. In January 2013, GQ placed her on its cover, featuring her atop its \"100 Sexiest Women of the 21st Century\" list. VH1 listed her at number 1 on its 100 Sexiest Artists list. Several wax figures of Beyoncé are found at Madame Tussauds Wax Museums in major cities around the world, including New York, Washington, D.C., Amsterdam, Bangkok, Hollywood and Sydney.\n",
      "Question: in September 2010, what career area did Beyonce start exploring? \n",
      "\n",
      "Context:  Beyoncé has won 20 Grammy Awards, both as a solo artist and member of Destiny's Child, making her the second most honored female artist by the Grammys, behind Alison Krauss and the most nominated woman in Grammy Award history with 52 nominations. \"Single Ladies (Put a Ring on It)\" won Song of the Year in 2010 while \"Say My Name\" and \"Crazy in Love\" had previously won Best R&B Song. Dangerously in Love, B'Day and I Am... Sasha Fierce have all won Best Contemporary R&B Album. Beyoncé set the record for the most Grammy awards won by a female artist in one night in 2010 when she won six awards, breaking the tie she previously held with Alicia Keys, Norah Jones, Alison Krauss, and Amy Winehouse, with Adele equaling this in 2012. Following her role in Dreamgirls she was nominated for Best Original Song for \"Listen\" and Best Actress at the Golden Globe Awards, and Outstanding Actress in a Motion Picture at the NAACP Image Awards. Beyoncé won two awards at the Broadcast Film Critics Association Awards 2006; Best Song for \"Listen\" and Best Original Soundtrack for Dreamgirls: Music from the Motion Picture.\n",
      "Question: Who is the only other woman with more Grammy awards than Beyonce? \n",
      "\n",
      "Context:  After Hurricane Katrina in 2005, Beyoncé and Rowland founded the Survivor Foundation to provide transitional housing for victims in the Houston area, to which Beyoncé contributed an initial $250,000. The foundation has since expanded to work with other charities in the city, and also provided relief following Hurricane Ike three years later.\n",
      "Question: How much did Beyonce initially contribute to the foundation? \n",
      "\n",
      "Context:  Montana is home to a diverse array of fauna that includes 14 amphibian, 90 fish, 117 mammal, 20 reptile and 427 bird species. Additionally, there are over 10,000 invertebrate species, including 180 mollusks and 30 crustaceans. Montana has the largest grizzly bear population in the lower 48 states. Montana hosts five federally endangered species–black-footed ferret, whooping crane, least tern, pallid sturgeon and white sturgeon and seven threatened species including the grizzly bear, Canadian lynx and bull trout. The Montana Department of Fish, Wildlife and Parks manages fishing and hunting seasons for at least 17 species of game fish including seven species of trout, walleye and smallmouth bass and at least 29 species of game birds and animals including ring-neck pheasant, grey partridge, elk, pronghorn antelope, mule deer, whitetail deer, gray wolf and bighorn sheep.\n",
      "Question: How many species of game fish have hunting seasons? \n",
      "\n",
      "Context:  In 2007 the European Court of Human Rights (ECHR), noted in its judgement on Jorgic v. Germany case that in 1992 the majority of legal scholars took the narrow view that \"intent to destroy\" in the CPPCG meant the intended physical-biological destruction of the protected group and that this was still the majority opinion. But the ECHR also noted that a minority took a broader view and did not consider biological-physical destruction was necessary as the intent to destroy a national, racial, religious or ethnic group was enough to qualify as genocide.\n",
      "Question: What form of destruction was considered too limited by a smaller group of experts? \n",
      "\n",
      "Context:  There has been much debate over categorizing the situation in Darfur as genocide. The ongoing conflict in Darfur, Sudan, which started in 2003, was declared a \"genocide\" by United States Secretary of State Colin Powell on 9 September 2004 in testimony before the Senate Foreign Relations Committee. Since that time however, no other permanent member of the UN Security Council followed suit. In fact, in January 2005, an International Commission of Inquiry on Darfur, authorized by UN Security Council Resolution 1564 of 2004, issued a report to the Secretary-General stating that \"the Government of the Sudan has not pursued a policy of genocide.\" Nevertheless, the Commission cautioned that \"The conclusion that no genocidal policy has been pursued and implemented in Darfur by the Government authorities, directly or through the militias under their control, should not be taken in any way as detracting from the gravity of the crimes perpetrated in that region. International offences such as the crimes against humanity and war crimes that have been committed in Darfur may be no less serious and heinous than genocide.\"\n",
      "Question: What has been widely debated as a possible act of genocide in Sudan? \n",
      "\n",
      "Context:  The majority of studies indicate antibiotics do interfere with contraceptive pills, such as clinical studies that suggest the failure rate of contraceptive pills caused by antibiotics is very low (about 1%). In cases where antibacterials have been suggested to affect the efficiency of birth control pills, such as for the broad-spectrum antibacterial rifampicin, these cases may be due to an increase in the activities of hepatic liver enzymes' causing increased breakdown of the pill's active ingredients. Effects on the intestinal flora, which might result in reduced absorption of estrogens in the colon, have also been suggested, but such suggestions have been inconclusive and controversial. Clinicians have recommended that extra contraceptive measures be applied during therapies using antibacterials that are suspected to interact with oral contraceptives.\n",
      "Question: What do antibiotics interfere with? \n",
      "\n",
      "Context:  Frédéric François Chopin (/ˈʃoʊpæn/; French pronunciation: ​[fʁe.de.ʁik fʁɑ̃.swa ʃɔ.pɛ̃]; 22 February or 1 March 1810 – 17 October 1849), born Fryderyk Franciszek Chopin,[n 1] was a Polish and French (by citizenship and birth of father) composer and a virtuoso pianist of the Romantic era, who wrote primarily for the solo piano. He gained and has maintained renown worldwide as one of the leading musicians of his era, whose \"poetic genius was based on a professional technique that was without equal in his generation.\" Chopin was born in what was then the Duchy of Warsaw, and grew up in Warsaw, which after 1815 became part of Congress Poland. A child prodigy, he completed his musical education and composed his earlier works in Warsaw before leaving Poland at the age of 20, less than a month before the outbreak of the November 1830 Uprising.\n",
      "Question: In what era of music did Chopin compose? \n",
      "\n",
      "Context:  Fryderyk may have had some piano instruction from his mother, but his first professional music tutor, from 1816 to 1821, was the Czech pianist Wojciech Żywny. His elder sister Ludwika also took lessons from Żywny, and occasionally played duets with her brother. It quickly became apparent that he was a child prodigy. By the age of seven Fryderyk had begun giving public concerts, and in 1817 he composed two polonaises, in G minor and B-flat major. His next work, a polonaise in A-flat major of 1821, dedicated to Żywny, is his earliest surviving musical manuscript.\n",
      "Question: How old was Chopin when he began to perform for the public? \n",
      "\n",
      "Context:  Chopin's successes as a composer and performer opened the door to western Europe for him, and on 2 November 1830, he set out, in the words of Zdzisław Jachimecki, \"into the wide world, with no very clearly defined aim, forever.\" With Woyciechowski, he headed for Austria, intending to go on to Italy. Later that month, in Warsaw, the November 1830 Uprising broke out, and Woyciechowski returned to Poland to enlist. Chopin, now alone in Vienna, was nostalgic for his homeland, and wrote to a friend, \"I curse the moment of my departure.\" When in September 1831 he learned, while travelling from Vienna to Paris, that the uprising had been crushed, he expressed his anguish in the pages of his private journal: \"Oh God! ... You are there, and yet you do not take vengeance!\" Jachimecki ascribes to these events the composer's maturing \"into an inspired national bard who intuited the past, present and future of his native Poland.\"\n",
      "Question: What geographicla region was opened for Chopin due to his composing and performances? \n",
      "\n",
      "Context:  In 1836, at a party hosted by Marie d'Agoult, Chopin met the French author George Sand (born [Amantine] Aurore [Lucile] Dupin). Short (under five feet, or 152 cm), dark, big-eyed and a cigar smoker, she initially repelled Chopin, who remarked, \"What an unattractive person la Sand is. Is she really a woman?\" However, by early 1837 Maria Wodzińska's mother had made it clear to Chopin in correspondence that a marriage with her daughter was unlikely to proceed. It is thought that she was influenced by his poor health and possibly also by rumours about his associations with women such as d'Agoult and Sand. Chopin finally placed the letters from Maria and her mother in a package on which he wrote, in Polish, \"My tragedy\". Sand, in a letter to Grzymała of June 1838, admitted strong feelings for the composer and debated whether to abandon a current affair in order to begin a relationship with Chopin; she asked Grzymała to assess Chopin's relationship with Maria Wodzińska, without realising that the affair, at least from Maria's side, was over.\n",
      "Question: What is the name of the author Chopin met at a gathering put on by Marie d'Agoult? \n",
      "\n",
      "Context:  Chopin's public popularity as a virtuoso began to wane, as did the number of his pupils, and this, together with the political strife and instability of the time, caused him to struggle financially. In February 1848, with the cellist Auguste Franchomme, he gave his last Paris concert, which included three movements of the Cello Sonata Op. 65.\n",
      "Question: Who did Chopin have at his last Parisian concert in 1848? \n",
      "\n",
      "Context:  With his health further deteriorating, Chopin desired to have a family member with him. In June 1849 his sister Ludwika came to Paris with her husband and daughter, and in September, supported by a loan from Jane Stirling, he took an apartment at Place Vendôme 12. After 15 October, when his condition took a marked turn for the worse, only a handful of his closest friends remained with him, although Viardot remarked sardonically that \"all the grand Parisian ladies considered it de rigueur to faint in his room.\"\n",
      "Question: Who accompanied Chopin's sister to Paris? \n",
      "\n",
      "Context:  Chopin's mazurkas and waltzes are all in straightforward ternary or episodic form, sometimes with a coda. The mazurkas often show more folk features than many of his other works, sometimes including modal scales and harmonies and the use of drone basses. However, some also show unusual sophistication, for example Op. 63 No. 3, which includes a canon at one beat's distance, a great rarity in music.\n",
      "Question: What does Chopin's Op. 63 No. 3 have that is rare? \n",
      "\n",
      "Context:  In 1207, the Mongol ruler Genghis Khan (r. 1206–1227) conquered and subjugated the ethnic Tangut state of the Western Xia (1038–1227). In the same year, he established diplomatic relations with Tibet by sending envoys there. The conquest of the Western Xia alarmed Tibetan rulers, who decided to pay tribute to the Mongols. However, when they ceased to pay tribute after Genghis Khan's death, his successor Ögedei Khan (r. 1229–1241) launched an invasion into Tibet.\n",
      "Question: Which ruler took Western Xia under their control? \n",
      "\n",
      "Context:  A. Tom Grunfeld says that Tsongkhapa claimed ill health in his refusal to appear at the Ming court, while Rossabi adds that Tsongkhapa cited the \"length and arduousness of the journey\" to China as another reason not to make an appearance. This first request by the Ming was made in 1407, but the Ming court sent another embassy in 1413, this one led by the eunuch Hou Xian (候顯; fl. 1403–1427), which was again refused by Tsongkhapa. Rossabi writes that Tsongkhapa did not want to entirely alienate the Ming court, so he sent his disciple Chosrje Shākya Yeshes to Nanjing in 1414 on his behalf, and upon his arrival in 1415 the Yongle Emperor bestowed upon him the title of \"State Teacher\"—the same title earlier awarded the Phagmodrupa ruler of Tibet. The Xuande Emperor (r. 1425–1435) even granted this disciple Chosrje Shākya Yeshes the title of a \"King\" (王). This title does not appear to have held any practical meaning, or to have given its holder any power, at Tsongkhapa's Ganden Monastery. Wylie notes that this—like the Karma Kargyu—cannot be seen as a reappointment of Mongol Yuan offices, since the Gelug school was created after the fall of the Yuan dynasty.\n",
      "Question: When was Chosrje Shākya Yeshes sent to Nanjing? \n",
      "\n",
      "Context:  China Daily, a CCP-controlled news organization since 1981, states in a 2008 article that although there were dynastic changes after Tibet was incorporated into the territory of Yuan dynasty's China in the 13th century, \"Tibet has remained under the jurisdiction of the central government of China.\" It also states that the Ming dynasty \"inherited the right to rule Tibet\" from the Yuan dynasty, and repeats the claims in the Mingshi about the Ming establishing two itinerant high commands over Tibet. China Daily states that the Ming handled Tibet's civil administration, appointed all leading officials of these administrative organs, and punished Tibetans who broke the law. The party-controlled People's Daily, the state-controlled Xinhua News Agency, and the state-controlled national television network China Central Television posted the same article that China Daily had, the only difference being their headlines and some additional text.\n",
      "Question: When was Tibet included into the territory of Yuan dynasty's China? \n",
      "\n",
      "Context:  In mid-2015, a new model of the iPod Touch was announced by Apple, and was officially released on the Apple store on July 15, 2015. The sixth generation iPod Touch includes a wide variety of spec improvements such as the upgraded A8 processor and higher-quality screen. The core is over 5 times faster than previous models and is built to be roughly on par with the iPhone 5S. It is available in 5 different colors: Space grey, pink, gold, silver and Product (red).\n",
      "Question: What type of processor does the current iPod Touch use? \n",
      "\n",
      "Context:  Some independent stereo manufacturers including JVC, Pioneer, Kenwood, Alpine, Sony, and Harman Kardon also have iPod-specific integration solutions. Alternative connection methods include adapter kits (that use the cassette deck or the CD changer port), audio input jacks, and FM transmitters such as the iTrip—although personal FM transmitters are illegal in some countries. Many car manufacturers have added audio input jacks as standard.\n",
      "Question: What type of transmitter is used in the iTrip? \n",
      "\n",
      "Context:  On August 24, 2006, Apple and Creative announced a broad settlement to end their legal disputes. Apple will pay Creative US$100 million for a paid-up license, to use Creative's awarded patent in all Apple products. As part of the agreement, Apple will recoup part of its payment, if Creative is successful in licensing the patent. Creative then announced its intention to produce iPod accessories by joining the Made for iPod program.\n",
      "Question: How much did Apple pay to Creative Technologies to settle their 2006 suit? \n",
      "\n",
      "Context:  The Legend of Zelda: Twilight Princess (Japanese: ゼルダの伝説 トワイライトプリンセス, Hepburn: Zeruda no Densetsu: Towairaito Purinsesu?) is an action-adventure game developed and published by Nintendo for the GameCube and Wii home video game consoles. It is the thirteenth installment in the The Legend of Zelda series. Originally planned for release on the GameCube in November 2005, Twilight Princess was delayed by Nintendo to allow its developers to refine the game, add more content, and port it to the Wii. The Wii version was released alongside the console in North America in November 2006, and in Japan, Europe, and Australia the following month. The GameCube version was released worldwide in December 2006.[b]\n",
      "Question: When was Twilight Princess launched in North America? \n",
      "\n",
      "Context:  Ganondorf then revives, and Midna teleports Link and Zelda outside the castle so she can hold him off with the Fused Shadows. However, as Hyrule Castle collapses, it is revealed that Ganondorf was victorious as he crushes Midna's helmet. Ganondorf engages Link on horseback, and, assisted by Zelda and the Light Spirits, Link eventually knocks Ganondorf off his horse and they duel on foot before Link strikes down Ganondorf and plunges the Master Sword into his chest. With Ganondorf dead, the Light Spirits not only bring Midna back to life, but restore her to her true form. After bidding farewell to Link and Zelda, Midna returns home before destroying the Mirror of Twilight with a tear to maintain balance between Hyrule and the Twilight Realm. Near the end, as Hyrule Castle is rebuilt, Link is shown leaving Ordon Village heading to parts unknown.\n",
      "Question: What does Midna destroy? \n",
      "\n",
      "Context:  Twilight Princess received the awards for Best Artistic Design, Best Original Score, and Best Use of Sound from IGN for its GameCube version. Both IGN and Nintendo Power gave Twilight Princess the awards for Best Graphics and Best Story. Twilight Princess received Game of the Year awards from GameTrailers, 1UP.com, Electronic Gaming Monthly, Game Informer, Games Radar, GameSpy, Spacey Awards, X-Play and Nintendo Power. It was also given awards for Best Adventure Game from the Game Critics Awards, X-Play, IGN, GameTrailers, 1UP.com, and Nintendo Power. The game was considered the Best Console Game by the Game Critics Awards and GameSpy. The game placed 16th in Official Nintendo Magazine's list of the 100 Greatest Nintendo Games of All Time. IGN ranked the game as the 4th-best Wii game. Nintendo Power ranked the game as the third-best game to be released on a Nintendo system in the 2000s decade.\n",
      "Question: What place did the game take in Nintendo's Official list of 100 Greatest Nintendo Games of All Time? \n",
      "\n",
      "Context:  Despite being an original story, Spectre draws on Ian Fleming's source material, most notably in the character of Franz Oberhauser, played by Christoph Waltz. Oberhauser shares his name with Hannes Oberhauser, a background character in the short story \"Octopussy\" from the Octopussy and The Living Daylights collection, and who is named in the film as having been a temporary legal guardian of a young Bond in 1983. Similarly, Charmian Bond is shown to have been his full-time guardian, observing the back story established by Fleming. With the acquisition of the rights to Spectre and its associated characters, screenwriters Neal Purvis and Robert Wade revealed that the film would provide a minor retcon to the continuity of the previous films, with the Quantum organisation alluded to in Casino Royale and introduced in Quantum of Solace reimagined as a division within Spectre rather than an independent organisation.\n",
      "Question: What is the name of the short story in which Hannes Oberhauser appeared? \n",
      "\n",
      "Context:  Thomas Newman returned as Spectre's composer. Rather than composing the score once the film had moved into post-production, Newman worked during filming. The theatrical trailer released in July 2015 contained a rendition of John Barry's On Her Majesty's Secret Service theme. Mendes revealed that the final film would have more than one hundred minutes of music. The soundtrack album was released on 23 October 2015 in the UK and 6 November 2015 in the USA on the Decca Records label.\n",
      "Question: Which record label was the soundtrack album released on? \n",
      "\n",
      "Context:  It is also known as the Wenchuan earthquake (Chinese: 汶川大地震; pinyin: Wènchuān dà dìzhèn; literally: \"Great Wenchuan earthquake\"), after the location of the earthquake's epicenter, Wenchuan County, Sichuan. The epicenter was 80 kilometres (50 mi) west-northwest of Chengdu, the provincial capital, with a focal depth of 19 km (12 mi). The earthquake was also felt in nearby countries and as far away as both Beijing and Shanghai—1,500 km (930 mi) and 1,700 km (1,060 mi) away—where office buildings swayed with the tremor. Strong aftershocks, some exceeding magnitude 6, continued to hit the area even months after the main quake, causing new casualties and damage.\n",
      "Question: What was the focal depth of the quake? \n",
      "\n",
      "Context:  All of the highways into Wenchuan, and others throughout the province, were damaged, resulting in delayed arrival of the rescue troops. In Beichuan County, 80% of the buildings collapsed according to Xinhua News. In the city of Shifang, the collapse of two chemical plants led to leakage of some 80 tons of liquid ammonia, with hundreds of people reported buried. In the city of Dujiangyan, south-east of the epicenter, a whole school collapsed with 900 students buried and fewer than 60 survived. The Juyuan Middle School, where many teenagers were buried, was excavated by civilians and cranes. Dujiangyan is home of the Dujiangyan Irrigation System, an ancient water diversion project which is still in use and is a UNESCO World Heritage Site. The project's famous Fish Mouth was cracked but not severely damaged otherwise.\n",
      "Question: What leaked liquid ammonia in Shifang? \n",
      "\n",
      "Context:  In the days following the disaster, an international reconnaissance team of engineers was dispatched to the region to make a detailed preliminary survey of damaged buildings. Their findings show a variety of reasons why many constructions failed to withstand the earthquake.\n",
      "Question: What did the team of engineers do? \n",
      "\n",
      "Context:  In 2002, Chinese geologist Chen Xuezhong published a Seismic Risk Analysis study in which he came to the conclusion that beginning with 2003, attention should be paid to the possibility of an earthquake with a magnitude of over 7.0 occurring in Sichuan region. He based his study on statistical correlation. That Sichuan is a seismically active area has been discussed for years prior to the quake, though few studies point to a specific date and time.\n",
      "Question: Who published a Seismic Risk Analysis Study? \n",
      "\n",
      "Context:  On the evening of May 18, CCTV-1 hosted a special four-hour program called The Giving of Love (simplified Chinese: 爱的奉献; traditional Chinese: 愛的奉獻), hosted by regulars from the CCTV New Year's Gala and round-the-clock coverage anchor Bai Yansong. It was attended by a wide range of entertainment, literary, business and political figures from mainland China, Hong Kong, Singapore and Taiwan. Donations of the evening totalled 1.5 billion Chinese Yuan (~US$208 million). Of the donations, CCTV gave the biggest corporate contribution at ¥50 million. Almost at the same time in Taiwan, a similarly themed programme was on air hosted by the sitting president Ma Ying-jeou. In June, Hong Kong actor Jackie Chan, who donated $1.57 million to the victims, made a music video alongside other artists entitled \"Promise\"; the song was composed by Andy Lau. The Artistes 512 Fund Raising Campaign, an 8-hour fundraising marathon, was held on June 1 in Hong Kong; it was attended by some 200 Sinosphere musicians and celebrities. In Singapore, MediaCorp Channel 8 hosted a 'live' programme 让爱川流不息 to raise funds for the victims.\n",
      "Question: How much did actor Jackie Chan donate? \n",
      "\n",
      "Context:  On May 15, 2008 Geoffery York of the Globeandmail.com reported that the shoddily constructed buildings are commonly called \"tofu buildings\" because builders cut corners by replacing steel rods with thin iron wires for concrete re-inforcement; using inferior grade cement, if any at all; and using fewer bricks than they should. One local was quoted in the article as saying that \"the supervising agencies did not check to see if it met the national standards.\"\n",
      "Question: What did builder's use in place of steel rods as re-inforcement? \n",
      "\n",
      "Context:  New York grew in importance as a trading port while under British rule in the early 1700s. It also became a center of slavery, with 42% of households holding slaves by 1730, more than any other city other than Charleston, South Carolina. Most slaveholders held a few or several domestic slaves, but others hired them out to work at labor. Slavery became integrally tied to New York's economy through the labor of slaves throughout the port, and the banks and shipping tied to the South. Discovery of the African Burying Ground in the 1990s, during construction of a new federal courthouse near Foley Square, revealed that tens of thousands of Africans had been buried in the area in the colonial years.\n",
      "Question: What was being built that resulted in the discovery of the African Burial Ground? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# L'unica cosa che si potrebbe rimuovere è la fonetica, ma è poco presente. Inoltre facendolo bisognerebbe aggiornare gli indici\n",
    "\n",
    "# Some examples of contexts and questions:\n",
    "for i in range(0, 4000, 100):\n",
    "    # print('Title:   ', title_list[df['title_index'][i]])\n",
    "    print('Context: ', context_list[df['context_index'][i]])\n",
    "    print('Question:', df['question'][i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train paragraphs: 284\n",
      "Number of validation paragraphs: 70\n",
      "Number of test paragraphs: 88\n",
      "\n",
      "Number of train samples: 57451\n",
      "Number of validation samples: 12921\n",
      "Number of test samples: 17227\n"
     ]
    }
   ],
   "source": [
    "# Define split ratios\n",
    "test_ratio = 0.2\n",
    "val_ratio = 0.2\n",
    "\n",
    "# Build array of paragraphs indexes and shuffle them\n",
    "paragraph_indexes = df['paragraph_index'].unique()\n",
    "np.random.shuffle(paragraph_indexes)\n",
    "n_samples = len(paragraph_indexes)\n",
    "\n",
    "# Reserve indexes for test set\n",
    "test_size = int(test_ratio * n_samples)\n",
    "train_val_size = n_samples - test_size\n",
    "test_indexes = paragraph_indexes[-test_size:]\n",
    "# Reserve indexes for validation set\n",
    "val_size = int(val_ratio * train_val_size)\n",
    "train_size = train_val_size - val_size\n",
    "val_indexes = paragraph_indexes[-(test_size + val_size):-test_size]\n",
    "# Reserve indexes for training set\n",
    "train_indexes = paragraph_indexes[:train_size]\n",
    "\n",
    "assert train_size == len(train_indexes), 'Something went wrong with train set slicing'\n",
    "assert val_size == len(val_indexes), 'Something went wrong with val set slicing'\n",
    "assert test_size == len(test_indexes), 'Something went wrong with test set slicing'\n",
    "\n",
    "print('Number of train paragraphs:', train_size)\n",
    "print('Number of validation paragraphs:', val_size)\n",
    "print('Number of test paragraphs:', test_size)\n",
    "\n",
    "# Split dataframe\n",
    "df_train = df[np.in1d(df['paragraph_index'], train_indexes)]\n",
    "df_val = df[np.in1d(df['paragraph_index'], val_indexes)]\n",
    "df_test = df[np.in1d(df['paragraph_index'], test_indexes)]\n",
    "\n",
    "print('\\nNumber of train samples:', len(df_train))\n",
    "print('Number of validation samples:', len(df_val))\n",
    "print('Number of test samples:', len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe model...\n",
      "\n",
      "Download completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Downloading GloVe model...')\n",
    "emb_dim = 50\n",
    "glove_model = gloader.load('glove-wiki-gigaword-' + str(emb_dim))\n",
    "print('\\nDownload completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_train = pd.concat([df_train , df_val], axis=0) \n",
    "    df_val = df_test\n",
    "    \n",
    "if True:\n",
    "    df_train = df_train[:1000]\n",
    "    df_val = df_val[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training corpus... [0.831 s]\n",
      "Tokenizing validation corpus... [0.260 s]\n",
      "Tokenizing test corpus... [13.594 s]\n",
      "--------------------------------------------------\n",
      "Words in training set: 6290\n",
      "Words in validation set: 7722\n",
      "Words in test set: 49581\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(df: pd.DataFrame, context_list: List[str]):\n",
    "    twt = TreebankWordTokenizer()\n",
    "    \n",
    "    t_start = time()\n",
    "    # Retrieve contexts\n",
    "    contexts = df['context_index'].apply(lambda x: context_list[x])\n",
    "    # Tokenize both contexts and queries\n",
    "    x_ctx = contexts.apply(lambda x: twt.tokenize(x)).tolist()\n",
    "    x_qry = df['question'].apply(lambda x: twt.tokenize(x)).tolist()\n",
    "    # Get indexes to start_end characters\n",
    "    y_char = [(start, end) for start, end in zip(df['answer_start'].tolist(), df['answer_end'].tolist())]\n",
    "    # Get spans of tokens\n",
    "    spans_list = contexts.apply(lambda x: twt.span_tokenize(x)).tolist()\n",
    "    # Convert indexes s.t. the point to start/end tokens\n",
    "    y = []\n",
    "    for spans, (char_start, char_end) in zip(spans_list, y_char):\n",
    "        token_start, token_end = None, None\n",
    "        for i, span in enumerate(spans):\n",
    "            if span[0] <= char_start <= span[1]:\n",
    "                token_start = i\n",
    "            if span[0] <= char_end <= span[1]:\n",
    "                token_end = i\n",
    "        y.append((token_start, token_end))\n",
    "    print(f'[{time() - t_start:.3f} s]')\n",
    "    \n",
    "    return x_ctx, x_qry, y\n",
    "\n",
    "def build_vocabulary(corpus: List[List[str]],\n",
    "                     old_word_listing: Optional[List[str]] = None) -> (Dict[int, str], Dict[int, str], List[str]):\n",
    "    flat_tokens = [x for sub in corpus for x in sub]\n",
    "    \n",
    "    if old_word_listing is None:  # standard case\n",
    "        word_listing = [PAD] + list(OrderedDict.fromkeys(flat_tokens))\n",
    "    else:  # case in which we extend an already existing vocabulary\n",
    "        word_listing = list(OrderedDict.fromkeys(old_word_listing + flat_tokens))\n",
    "        \n",
    "    idx_to_word = {i: w for i, w in enumerate(word_listing)}\n",
    "    word_to_idx = {w: i for i, w in enumerate(word_listing)}\n",
    "\n",
    "    return idx_to_word, word_to_idx, word_listing\n",
    "\n",
    "# Tokenize corpus\n",
    "print('Tokenizing training corpus...', end=' ')\n",
    "X_trainC, X_trainQ, Y_train = tokenize_corpus(df_train, context_list)\n",
    "train_corpus = X_trainC + X_trainQ\n",
    "\n",
    "print('Tokenizing validation corpus...', end=' ')\n",
    "X_valC, X_valQ, Y_val = tokenize_corpus(df_val, context_list)\n",
    "val_corpus = X_valC + X_valQ\n",
    "\n",
    "print('Tokenizing test corpus...', end=' ')\n",
    "X_testC, X_testQ, Y_test = tokenize_corpus(df_test, context_list)\n",
    "test_corpus = X_testC + X_testQ\n",
    "\n",
    "# Get word and char mappings for each set\n",
    "train_i2w, train_w2i, train_wl = build_vocabulary(train_corpus)\n",
    "val_i2w, val_w2i, val_wl = build_vocabulary(val_corpus, train_wl)\n",
    "test_i2w, test_w2i, test_wl = build_vocabulary(test_corpus, val_wl)\n",
    "\n",
    "print('-' * 50)\n",
    "print('Words in training set:', len(train_wl))\n",
    "print('Words in validation set:', len(val_wl))\n",
    "print('Words in test set:', len(test_wl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Saint', 'Bernadette']\n",
      "['To', 'whom', 'did', 'the', 'Virgin', 'Mary', 'allegedly', 'appear', 'in', '1858', 'in', 'Lourdes', 'France', '?']\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "\n",
    "print(X_trainC[x][Y_train[x][0]:Y_train[x][1]])\n",
    "print(X_trainQ[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms in training set: 2255 (35.85%)\n",
      "Total OOV terms in validation set: 2763 (35.78%)\n",
      "Total OOV terms in test set: 26974 (54.40%)\n"
     ]
    }
   ],
   "source": [
    "train_oov_words = [word for word in train_wl if word not in glove_model.vocab and word != PAD]\n",
    "val_oov_words = [word for word in val_wl if word not in glove_model.vocab and word != PAD]\n",
    "test_oov_words = [word for word in test_wl if word not in glove_model.vocab and word != PAD]\n",
    "\n",
    "print(f'Total OOV terms in training set: {len(train_oov_words)} ({float(len(train_oov_words)) / len(train_wl) * 100:.2f}%)')\n",
    "print(f'Total OOV terms in validation set: {len(val_oov_words)} ({float(len(val_oov_words)) / len(val_wl) * 100:.2f}%)')\n",
    "print(f'Total OOV terms in test set: {len(test_oov_words)} ({float(len(test_oov_words)) / len(test_wl) * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word embedding matrix (training set): (6290, 50)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word embedding matrix (validation set): (7722, 50)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word embedding matrix (test set): (49581, 50)\n"
     ]
    }
   ],
   "source": [
    "def build_word_embedding_matrix(embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors,\n",
    "                                word_to_idx: Dict[str, int],\n",
    "                                oov_words: List[str],\n",
    "                                old_word_embedding_matrix: Optional[np.ndarray] = None):\n",
    "    # Initialize embedding matrix with all zeros\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_model.vector_size))\n",
    "    \n",
    "    # Analyze embeddings to get mean and standard deviation\n",
    "    mean_list, std_list = [], []\n",
    "    for word in tqdm(word_to_idx.keys(), leave=False):\n",
    "        if word not in oov_words and word != PAD:\n",
    "            embed = embedding_model[word]\n",
    "            # Compute mean and std\n",
    "            mean_list.append(np.mean(embed))\n",
    "            std_list.append(np.std(embed))\n",
    "\n",
    "    embedding_mean = mean(mean_list)\n",
    "    embedding_std = mean(std_list)\n",
    "\n",
    "    for word, idx in tqdm(word_to_idx.items(), leave=False):\n",
    "        # If word is PAD no action is performed (it will be assigned the zero vector)\n",
    "        if word not in oov_words and word != PAD:\n",
    "            embedding_matrix[idx] = embedding_model[word]\n",
    "        elif word in oov_words:\n",
    "            oov_idx = word_to_idx[word]\n",
    "            if old_word_embedding_matrix is None or oov_idx >= len(old_word_embedding_matrix):\n",
    "                embedding_matrix[idx] = np.random.normal(loc=embedding_mean, scale=embedding_std, size=embedding_model.vector_size)\n",
    "            else:\n",
    "                embedding_matrix[idx] = old_word_embedding_matrix[oov_idx]\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "# Build word embedding matrix based only on the training set (for training)\n",
    "train_emb_mtx = build_word_embedding_matrix(glove_model, train_w2i, train_oov_words)\n",
    "print('Shape of word embedding matrix (training set):', train_emb_mtx.shape)\n",
    "\n",
    "# Build word embedding matrix based on training + validation set (for validation)\n",
    "val_emb_mtx = build_word_embedding_matrix(glove_model, val_w2i, val_oov_words, train_emb_mtx)\n",
    "print('Shape of word embedding matrix (validation set):', val_emb_mtx.shape)\n",
    "\n",
    "# Build word embedding matrix based on training + validation + test set (for test)\n",
    "test_emb_mtx = build_word_embedding_matrix(glove_model, test_w2i, test_oov_words, val_emb_mtx)\n",
    "print('Shape of word embedding matrix (test set):', test_emb_mtx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of char embedding matrix (training set): (101, 100)\n"
     ]
    }
   ],
   "source": [
    "def build_char_embedding_matrix(corpus: List[str],\n",
    "                                enc_dim: Optional[int] = 100):\n",
    "    # Flatten to obtain single characters\n",
    "    flat_chars = [c for sent in corpus for word in sent for c in word]\n",
    "    \n",
    "    # Sort characters by occurrences\n",
    "    unique_chars = Counter(flat_chars)\n",
    "    char_listing = sorted(unique_chars, key=unique_chars.get, reverse=True)\n",
    "    # Select only the enc_dim most frequent ones\n",
    "    if len(char_listing) > enc_dim - 1:\n",
    "        char_listing = char_listing[:enc_dim - 1]\n",
    "    char_listing = [PAD] + char_listing + [UNK]  # add PAD and UNK tokens\n",
    "    \n",
    "    idx_to_char = {i: c for i, c in enumerate(char_listing)}\n",
    "    char_to_idx = {c: i for i, c in enumerate(char_listing)}\n",
    "    \n",
    "    # Create one-hot vectors, reserving the last one for UNK (0...0, 1)\n",
    "    one_hot_chars = np.zeros((len(char_listing) - 1, enc_dim))\n",
    "    np.fill_diagonal(one_hot_chars, 1)\n",
    "    one_hot_chars = np.vstack([np.zeros((1, enc_dim)), one_hot_chars])  # stack zero vector on top for PAD\n",
    "    \n",
    "    return idx_to_char, char_to_idx, char_listing, one_hot_chars\n",
    "\n",
    "# Build char embedding matrix based only on the training set, and use it for validation and test too:\n",
    "# in fact, we can assume that characters appear uniformly in the three splits;\n",
    "# for those rare case in which this does not happen, we assign the UNK vector\n",
    "i2c, c2i, cl, char_emb_mtx = build_char_embedding_matrix(train_corpus)\n",
    "print('Shape of char embedding matrix (training set):', char_emb_mtx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bidaf_train_utils import training_loop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "\n",
    "def plot_history(history):    \n",
    "    # this function is simply used to plot and save the image (and the dictionary) about the train and val loss and accuracy during the training\n",
    "    \n",
    "    \n",
    "    fig1, axes = plt.subplots(nrows=1, ncols=1, figsize=(7.5, 5))\n",
    "    plt.suptitle('loss', size='xx-large')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.])\n",
    "\n",
    "    axes.plot(history['loss'], label='train_loss')\n",
    "    axes.plot(history['val_loss'], label='val_loss')\n",
    "    axes.set_title('loss')\n",
    "    axes.set(xlabel='# Epochs')\n",
    "    axes.grid()\n",
    "    axes.legend();\n",
    "\n",
    "    fig2, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "    plt.suptitle('scores', size='xx-large')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    axes[0].plot(history['exact_score'], label='train_exact_score')\n",
    "    axes[0].plot(history['val_exact_score'], label='val_exact_score')\n",
    "    axes[0].set_title('exact_score')\n",
    "    axes[0].set(xlabel='# Epochs')\n",
    "    axes[0].grid()\n",
    "    axes[0].legend();\n",
    "\n",
    "    axes[1].plot(history['f1_score'], label='train_f1_score')\n",
    "    axes[1].plot(history['val_f1_score'], label='val_f1_score')\n",
    "    axes[1].set_title('f1_score')\n",
    "    axes[1].set(xlabel='# Epochs')\n",
    "    axes[1].grid()\n",
    "    axes[1].legend();\n",
    "    \n",
    "    fig3, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "    plt.suptitle('distances', size='xx-large')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    axes[0].plot(history['distance_end'], label='train_distance_end')\n",
    "    axes[0].plot(history['val_distance_end'], label='val_distance_end')\n",
    "    axes[0].set_title('distance_end')\n",
    "    axes[0].set(xlabel='# Epochs')\n",
    "    axes[0].grid()\n",
    "    axes[0].legend();\n",
    "\n",
    "    axes[1].plot(history['distance_start'], label='train_distance_start')\n",
    "    axes[1].plot(history['val_distance_start'], label='val_distance_start')\n",
    "    axes[1].set_title('distance_start')\n",
    "    axes[1].set(xlabel='# Epochs')\n",
    "    axes[1].grid()\n",
    "    axes[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jojonki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu memory before another training:\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.bidaf import BiDAF\n",
    "from model.char_embedder import CharEmbedder\n",
    "from model.word_embedder import WordEmbedder\n",
    "from model.tensor_maker import TensorMaker\n",
    "\n",
    "char_embedder = CharEmbedder(c_embd_size = 8, vocab_size_c = len(c2i), out_chs = 100, filters = [[1, 5]])\n",
    "\n",
    "train_word_embedder = WordEmbedder(init_emb = torch.FloatTensor(train_emb_mtx))\n",
    "val_word_embedder = WordEmbedder(init_emb = torch.FloatTensor(val_emb_mtx))\n",
    "\n",
    "model_bidaf = BiDAF(char_embedder, train_word_embedder, val_word_embedder, use_constraint = True).to(DEVICE)\n",
    "\n",
    "train_tensor_maker = TensorMaker(train_w2i, c2i, device=DEVICE)\n",
    "val_tensor_maker = TensorMaker(val_w2i, c2i, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.squad_utils import squad_loss\n",
    "\n",
    "train_data = to_list_of_tuples((X_trainC, X_trainQ, Y_train))\n",
    "val_data = to_list_of_tuples((X_valC, X_valQ, Y_val))\n",
    "\n",
    "EP = 30\n",
    "BS = 8\n",
    "\n",
    "#optimizer = torch.optim.Adam(model_bidaf.parameters(), lr=5e-3)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_bidaf.parameters()))\n",
    "#optimizer = torch.optim.Adadelta(model_bidaf.parameters(), lr=0.5, rho=0.999, eps=1e-06, weight_decay=0) # slower in time and in loss\n",
    "criterion = squad_loss\n",
    "\n",
    "\n",
    "history_noconst_jojonki_nohigh_1k = training_loop(model=model_bidaf,\n",
    "                        train_data=train_data,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=EP,\n",
    "                        batch_size=BS,\n",
    "                        criterion=criterion,\n",
    "                        train_tensor_maker=train_tensor_maker,\n",
    "                        val_tensor_maker=val_tensor_maker,\n",
    "                        val_data=val_data,\n",
    "                        early_stopping=True,\n",
    "                        patience = 15,\n",
    "                        checkpoint_path='bidaf_gru_noconstraint.pt',\n",
    "                        mix_scale = True)\n",
    "\n",
    "# eps =1e-7, #jojonki, # use_constraint, # BS = 8, #1ktrain, 3kval, lr=5e-3, patience = 30, EP = 50, emb = 100\n",
    "# adam optimizer # train+val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history_noconst_jojonki_nohigh_1k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu memory before another training:\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.bidaf import BiDAF\n",
    "from model.char_embedder import CharEmbedder\n",
    "from model.word_embedder import WordEmbedder\n",
    "from model.tensor_maker import TensorMaker\n",
    "\n",
    "char_embedder = CharEmbedder(init_emb = torch.FloatTensor(char_emb_mtx),\n",
    "                             out_char_emb_dim = emb_dim,\n",
    "                             hidden_dim = 64,\n",
    "                             input_channels = 1,\n",
    "                             output_channels = 100,\n",
    "                             kernel_height = 5,\n",
    "                             trainable = False)\n",
    "\n",
    "train_word_embedder = WordEmbedder(init_emb = torch.FloatTensor(train_emb_mtx))\n",
    "val_word_embedder = WordEmbedder(init_emb = torch.FloatTensor(val_emb_mtx))\n",
    "model_bidaf = BiDAF(char_embedder, train_word_embedder, val_word_embedder, use_constraint = True, use_dropout = False).to(DEVICE)\n",
    "train_tensor_maker = TensorMaker(train_w2i, c2i, device=DEVICE)\n",
    "val_tensor_maker = TensorMaker(val_w2i, c2i, device=DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d3f125227842d3882b894a06c46410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-482a70ff9df4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquad_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m history_drop = training_loop(model=model_bidaf,\n\u001b[0m\u001b[0;32m     15\u001b[0m                         \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\UNI AI\\2year_1semester\\NLP\\final_project\\qa-nlp\\qa-nlp\\utils\\bidaf_train_utils.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(model, train_data, optimizer, epochs, batch_size, criterion, train_tensor_maker, val_tensor_maker, lr_scheduler, val_data, early_stopping, patience, tolerance, checkpoint_path, verbose, seed, mix_scale)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         train_loss, train_distance_start, train_distance_end, exact_score, f1_score = train(model, train_data, batch_size, criterion,\n\u001b[0m\u001b[0;32m    241\u001b[0m                                                                      optimizer, train_tensor_maker, verbose, scaler)\n\u001b[0;32m    242\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\UNI AI\\2year_1semester\\NLP\\final_project\\qa-nlp\\qa-nlp\\utils\\bidaf_train_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data, batch_size, criterion, optimizer, tensor_maker, verbose, scaler)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_soft_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_soft_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# https://pytorch.org/docs/stable/notes/amp_examples.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# https://pytorch.org/docs/stable/notes/amp_examples.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# https://pytorch.org/docs/stable/notes/amp_examples.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\97and\\anaconda3\\envs\\colab_friend\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\97and\\anaconda3\\envs\\colab_friend\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.squad_utils import squad_loss\n",
    "\n",
    "train_data = to_list_of_tuples((X_trainC, X_trainQ, Y_train))\n",
    "val_data = to_list_of_tuples((X_valC, X_valQ, Y_val))\n",
    "\n",
    "EP = 30\n",
    "BS = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_bidaf.parameters()))\n",
    "# optimizer = torch.optim.Adam(model_bidaf.parameters(), lr=5e-3)\n",
    "# optimizer = torch.optim.Adadelta(model_bidaf.parameters(), lr=0.5, rho=0.999, eps=1e-06, weight_decay=0) # slower in time and in loss\n",
    "criterion = squad_loss\n",
    "\n",
    "history_drop = training_loop(model=model_bidaf,\n",
    "                        train_data=train_data,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=EP,\n",
    "                        batch_size=BS,\n",
    "                        criterion=criterion,\n",
    "                        train_tensor_maker=train_tensor_maker,\n",
    "                        val_tensor_maker=val_tensor_maker,\n",
    "                        val_data=val_data,\n",
    "                        early_stopping=True,\n",
    "                        patience = 15,\n",
    "                        checkpoint_path='bidaf_gru_constraint.pt',\n",
    "                        mix_scale = True)\n",
    "\n",
    "# p_end > p_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to do :\n",
    "\n",
    "- dropout\n",
    "- explanation for bad results: BS too small\n",
    "- our model: trainable embedder = True\n",
    "- torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "- propose distance as new metric (normalized wrt the number of characters in the context)\n",
    "\n",
    "\n",
    "\n",
    "our takes 2 hours per epoch\n",
    "\n",
    "jojonki takes \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
